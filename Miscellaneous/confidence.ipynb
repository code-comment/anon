{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install statistics\n",
    "from statistics import mean\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_file = 'results_5-GRAM/predictions_javadoc.txt'\n",
    "targets_file = 'results_5-GRAM/Score_Analysis/predictions_javadoc.txt.targets'\n",
    "scores_file = 'results_5-GRAM/Score_Analysis/predictions_javadoc.txt.scores'\n",
    "\n",
    "with open(predictions_file) as fread:\n",
    "    predictions = [item.strip().strip('\\n') for item in fread.readlines()]\n",
    "\n",
    "with open(targets_file) as fread:\n",
    "    targets = [item.strip().strip('\\n') for item in fread.readlines()]\n",
    "\n",
    "with open(scores_file) as fread:\n",
    "    scores = [item.strip().strip('\\n') for item in fread.readlines()] \n",
    "\n",
    "\n",
    "confidence_perfect = dict()\n",
    "confidence_wrong = dict()\n",
    "\n",
    "\n",
    "for i in range(1,16):\n",
    "    confidence_perfect[i] = []\n",
    "    confidence_wrong[i] = []\n",
    "\n",
    "    \n",
    "for token_len in range(1,16):\n",
    "    \n",
    "    #pred = os.path.join(base_path_perfect,'pred')\n",
    "    #target = os.path.join(base_path_perfect,'target')\n",
    "    \n",
    "   \n",
    "    dict_perfect = dict()\n",
    "    dict_non_perfect = dict()\n",
    "\n",
    "    for i in range(1, 11):\n",
    "        dict_perfect[i] = 0\n",
    "        dict_non_perfect[i] = 0\n",
    "\n",
    "    scores = [float(s) for s in scores]\n",
    "    \n",
    "    for x, y, z in zip(predictions, targets, scores):\n",
    "        \n",
    "        pred = x.strip().lower().split()\n",
    "        ref = y.strip().lower().split()\n",
    "\n",
    "\n",
    "        if len(ref)>=token_len and len(pred)>=token_len:\n",
    "            \n",
    "            pred = pred[0:token_len]\n",
    "            ref = ref[0:token_len]\n",
    "\n",
    "\n",
    "            likelihood = math.exp(z)\n",
    "            class_likelihood = get_class(likelihood)\n",
    "            if (''.join(pred) == ''.join(ref)):\n",
    "                dict_perfect[class_likelihood] += 1  \n",
    "                confidence_perfect[token_len].append(likelihood)\n",
    "            else:\n",
    "                dict_non_perfect[class_likelihood] += 1\n",
    "                confidence_wrong[token_len].append(likelihood)\n",
    "\n",
    "          \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "****************************************\n",
      "Perfect prediction average confidence token len 1: 0.46313836407723225\n",
      "Wrong prediction average confidence token len 1: 0.1579149688561501\n",
      "****************************************\n",
      "\n",
      "\n",
      "****************************************\n",
      "Perfect prediction average confidence token len 2: 0.48209913972927726\n",
      "Wrong prediction average confidence token len 2: 0.13950567867854055\n",
      "****************************************\n",
      "\n",
      "\n",
      "****************************************\n",
      "Perfect prediction average confidence token len 3: 0.4542881810475699\n",
      "Wrong prediction average confidence token len 3: 0.12368678826667028\n",
      "****************************************\n",
      "\n",
      "\n",
      "****************************************\n",
      "Perfect prediction average confidence token len 4: 0.43890817719987224\n",
      "Wrong prediction average confidence token len 4: 0.11127090053329082\n",
      "****************************************\n",
      "\n",
      "\n",
      "****************************************\n",
      "Perfect prediction average confidence token len 5: 0.41247210327843253\n",
      "Wrong prediction average confidence token len 5: 0.10079579703369027\n",
      "****************************************\n",
      "\n",
      "\n",
      "****************************************\n",
      "Perfect prediction average confidence token len 6: 0.3781741342089079\n",
      "Wrong prediction average confidence token len 6: 0.09168609774859651\n",
      "****************************************\n",
      "\n",
      "\n",
      "****************************************\n",
      "Perfect prediction average confidence token len 7: 0.33134675340736436\n",
      "Wrong prediction average confidence token len 7: 0.08439301490759747\n",
      "****************************************\n",
      "\n",
      "\n",
      "****************************************\n",
      "Perfect prediction average confidence token len 8: 0.28956083320671383\n",
      "Wrong prediction average confidence token len 8: 0.07883496998313368\n",
      "****************************************\n",
      "\n",
      "\n",
      "****************************************\n",
      "Perfect prediction average confidence token len 9: 0.24307348696261932\n",
      "Wrong prediction average confidence token len 9: 0.07737705975224418\n",
      "****************************************\n",
      "\n",
      "\n",
      "****************************************\n",
      "Perfect prediction average confidence token len 10: 0.19402914816501293\n",
      "Wrong prediction average confidence token len 10: 0.07296413720768853\n",
      "****************************************\n",
      "\n",
      "\n",
      "****************************************\n",
      "Perfect prediction average confidence token len 11: 0.1720964817507998\n",
      "Wrong prediction average confidence token len 11: 0.05818564681504419\n",
      "****************************************\n",
      "\n",
      "\n",
      "****************************************\n",
      "Perfect prediction average confidence token len 12: 0.14759775884636864\n",
      "Wrong prediction average confidence token len 12: 0.057209135172404335\n",
      "****************************************\n",
      "\n",
      "\n",
      "****************************************\n",
      "Perfect prediction average confidence token len 13: 0.1373385076412695\n",
      "Wrong prediction average confidence token len 13: 0.05486330950741409\n",
      "****************************************\n",
      "\n",
      "\n",
      "****************************************\n",
      "Perfect prediction average confidence token len 14: 0.12816985921200874\n",
      "Wrong prediction average confidence token len 14: 0.05633311842938672\n",
      "****************************************\n",
      "\n",
      "\n",
      "****************************************\n",
      "Perfect prediction average confidence token len 15: 0\n",
      "Wrong prediction average confidence token len 15: 0\n",
      "****************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,16):\n",
    "    \n",
    "    if len(confidence_perfect[i]) > 0 and len(confidence_wrong[i]) > 0:\n",
    "        print('\\n****************************************')\n",
    "        print('Perfect prediction average confidence token len {}: {}'.format(i,mean(confidence_perfect[i])))\n",
    "        print('Wrong prediction average confidence token len {}: {}'.format(i,mean(confidence_wrong[i])))\n",
    "        print('****************************************\\n')\n",
    "    \n",
    "    elif len(confidence_perfect[i]) > 0:\n",
    "        print('\\n****************************************')\n",
    "        print('Perfect prediction average confidence token len {}: {}'.format(i,confidence_perfect[i]))\n",
    "        print('Wrong prediction average confidence token len {}: {}'.format(i,0))\n",
    "        print('****************************************\\n')\n",
    "    \n",
    "    else:\n",
    "        print('\\n****************************************')\n",
    "        print('Perfect prediction average confidence token len {}: {}'.format(i,0))\n",
    "        print('Wrong prediction average confidence token len {}: {}'.format(i,0))\n",
    "        print('****************************************\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
